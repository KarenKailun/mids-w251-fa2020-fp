{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose detection from camera feed\n",
    "\n",
    "\n",
    "I adapted setup instructions from here: https://github.com/NVIDIA-AI-IOT/trt_pose\n",
    "* Used this for installing pytorch: https://forums.developer.nvidia.com/t/pytorch-for-jetpack-4-4-l4t-r32-4-3-in-jetson-xavier-nx/141455\n",
    "* I ended up having to install tensortRT as well\n",
    "\n",
    "Code is adapted from https://spyjetson.blogspot.com/2019/12/jetsonnano-human-pose-estimation-using.html\n",
    "\n",
    "The original code has functionality to draw the keypoints with connecting lines on the video, but I've ignored that. There are still vestiges of this in the code below, though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import trt_pose.coco\n",
    "import trt_pose.models\n",
    "import torch\n",
    "import torch2trt\n",
    "from torch2trt import TRTModule\n",
    "import time, sys\n",
    "import cv2\n",
    "import PIL.Image, PIL.ImageDraw, PIL.ImageFont\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "import argparse\n",
    "import os.path\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(img, key):\n",
    "    thickness = 5\n",
    "    w, h = img.size\n",
    "    draw = PIL.ImageDraw.Draw(img)\n",
    "    #draw Rankle -> RKnee (16-> 14)\n",
    "    if all(key[16]) and all(key[14]):\n",
    "        draw.line([ round(key[16][2] * w), round(key[16][1] * h), round(key[14][2] * w), round(key[14][1] * h)],width = thickness, fill=(51,51,204))\n",
    "    #draw RKnee -> Rhip (14-> 12)\n",
    "    if all(key[14]) and all(key[12]):\n",
    "        draw.line([ round(key[14][2] * w), round(key[14][1] * h), round(key[12][2] * w), round(key[12][1] * h)],width = thickness, fill=(51,51,204))\n",
    "    #draw Rhip -> Lhip (12-> 11)\n",
    "    if all(key[12]) and all(key[11]):\n",
    "        draw.line([ round(key[12][2] * w), round(key[12][1] * h), round(key[11][2] * w), round(key[11][1] * h)],width = thickness, fill=(51,51,204))\n",
    "    #draw Lhip -> Lknee (11-> 13)\n",
    "    if all(key[11]) and all(key[13]):\n",
    "        draw.line([ round(key[11][2] * w), round(key[11][1] * h), round(key[13][2] * w), round(key[13][1] * h)],width = thickness, fill=(51,51,204))\n",
    "    #draw Lknee -> Lankle (13-> 15)\n",
    "    if all(key[13]) and all(key[15]):\n",
    "        draw.line([ round(key[13][2] * w), round(key[13][1] * h), round(key[15][2] * w), round(key[15][1] * h)],width = thickness, fill=(51,51,204))\n",
    "\n",
    "    #draw Rwrist -> Relbow (10-> 8)\n",
    "    if all(key[10]) and all(key[8]):\n",
    "        draw.line([ round(key[10][2] * w), round(key[10][1] * h), round(key[8][2] * w), round(key[8][1] * h)],width = thickness, fill=(255,255,51))\n",
    "    #draw Relbow -> Rshoulder (8-> 6)\n",
    "    if all(key[8]) and all(key[6]):\n",
    "        draw.line([ round(key[8][2] * w), round(key[8][1] * h), round(key[6][2] * w), round(key[6][1] * h)],width = thickness, fill=(255,255,51))\n",
    "    #draw Rshoulder -> Lshoulder (6-> 5)\n",
    "    if all(key[6]) and all(key[5]):\n",
    "        draw.line([ round(key[6][2] * w), round(key[6][1] * h), round(key[5][2] * w), round(key[5][1] * h)],width = thickness, fill=(255,255,0))\n",
    "    #draw Lshoulder -> Lelbow (5-> 7)\n",
    "    if all(key[5]) and all(key[7]):\n",
    "        draw.line([ round(key[5][2] * w), round(key[5][1] * h), round(key[7][2] * w), round(key[7][1] * h)],width = thickness, fill=(51,255,51))\n",
    "    #draw Lelbow -> Lwrist (7-> 9)\n",
    "    if all(key[7]) and all(key[9]):\n",
    "        draw.line([ round(key[7][2] * w), round(key[7][1] * h), round(key[9][2] * w), round(key[9][1] * h)],width = thickness, fill=(51,255,51))\n",
    "\n",
    "    #draw Rshoulder -> RHip (6-> 12)\n",
    "    if all(key[6]) and all(key[12]):\n",
    "        draw.line([ round(key[6][2] * w), round(key[6][1] * h), round(key[12][2] * w), round(key[12][1] * h)],width = thickness, fill=(153,0,51))\n",
    "    #draw Lshoulder -> LHip (5-> 11)\n",
    "    if all(key[5]) and all(key[11]):\n",
    "        draw.line([ round(key[5][2] * w), round(key[5][1] * h), round(key[11][2] * w), round(key[11][1] * h)],width = thickness, fill=(153,0,51))\n",
    "\n",
    "\n",
    "    #draw nose -> Reye (0-> 2)\n",
    "    if all(key[0][1:]) and all(key[2]):\n",
    "        draw.line([ round(key[0][2] * w), round(key[0][1] * h), round(key[2][2] * w), round(key[2][1] * h)],width = thickness, fill=(219,0,219))\n",
    "\n",
    "    #draw Reye -> Rear (2-> 4)\n",
    "    if all(key[2]) and all(key[4]):\n",
    "        draw.line([ round(key[2][2] * w), round(key[2][1] * h), round(key[4][2] * w), round(key[4][1] * h)],width = thickness, fill=(219,0,219))\n",
    "\n",
    "    #draw nose -> Leye (0-> 1)\n",
    "    if all(key[0][1:]) and all(key[1]):\n",
    "        draw.line([ round(key[0][2] * w), round(key[0][1] * h), round(key[1][2] * w), round(key[1][1] * h)],width = thickness, fill=(219,0,219))\n",
    "\n",
    "    #draw Leye -> Lear (1-> 3)\n",
    "    if all(key[1]) and all(key[3]):\n",
    "        draw.line([ round(key[1][2] * w), round(key[1][1] * h), round(key[3][2] * w), round(key[3][1] * h)],width = thickness, fill=(219,0,219))\n",
    "\n",
    "    #draw nose -> neck (0-> 17)\n",
    "    if all(key[0][1:]) and all(key[17]):\n",
    "        draw.line([ round(key[0][2] * w), round(key[0][1] * h), round(key[17][2] * w), round(key[17][1] * h)],width = thickness, fill=(255,255,0))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "hnum: 0 based human index\n",
    "kpoint : keypoints (float type range : 0.0 ~ 1.0 ==> later multiply by image width, height\n",
    "'''\n",
    "def get_keypoint(humans, hnum, peaks):\n",
    "    #check invalid human index\n",
    "    kpoint = []\n",
    "    human = humans[0][hnum]\n",
    "    C = human.shape[0]\n",
    "    for j in range(C):\n",
    "        k = int(human[j])\n",
    "        if k >= 0:\n",
    "            peak = peaks[0][j][k]   # peak[1]:width, peak[0]:height\n",
    "            peak = (j, float(peak[0]), float(peak[1]))\n",
    "            kpoint.append(peak)\n",
    "            #print('index:%d : success [%5.3f, %5.3f]'%(j, peak[1], peak[2]) )\n",
    "        else:    \n",
    "            peak = (j, None, None)\n",
    "            kpoint.append(peak)\n",
    "            #print('index:%d : None %d'%(j, k) )\n",
    "    return kpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='TensorRT pose estimation run')\n",
    "# parser.add_argument('--model', type=str, default='resnet', help = 'resnet or densenet' )\n",
    "# parser.add_argument('--video', type=str, default='/home/spypiggy/src/test_images/video.avi', help = 'video file name' )\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../trt_pose/tasks/human_pose/human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parts = len(human_pose['keypoints'])\n",
    "num_links = len(human_pose['skeleton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ model = resnet--------\n"
     ]
    }
   ],
   "source": [
    "# if 'resnet' in args.model:\n",
    "print('------ model = resnet--------')\n",
    "MODEL_WEIGHTS = '../trt_pose/tasks/human_pose/resnet18_baseline_att_224x224_A_epoch_249.pth'\n",
    "OPTIMIZED_MODEL = '../trt_pose/tasks/human_pose/resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "model = trt_pose.models.resnet18_baseline_att(num_parts, 2 * num_links).cuda().eval()\n",
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "\n",
    "# else:    \n",
    "#     print('------ model = densenet--------')\n",
    "#     MODEL_WEIGHTS = 'densenet121_baseline_att_256x256_B_epoch_160.pth'\n",
    "#     OPTIMIZED_MODEL = 'densenet121_baseline_att_256x256_B_epoch_160_trt.pth'\n",
    "#     model = trt_pose.models.densenet121_baseline_att(num_parts, 2 * num_links).cuda().eval()\n",
    "#     WIDTH = 256\n",
    "#     HEIGHT = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.zeros((1, 3, HEIGHT, WIDTH)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Converting TensorRT models. This may takes several minutes...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(OPTIMIZED_MODEL) == False:\n",
    "    print('-- Converting TensorRT models. This may takes several minutes...')\n",
    "    model.load_state_dict(torch.load(MODEL_WEIGHTS))\n",
    "    model_trt = torch2trt.torch2trt(model, [data], fp16_mode=True, max_workspace_size=1<<25)\n",
    "    torch.save(model_trt.state_dict(), OPTIMIZED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "torch.cuda.current_stream().synchronize()\n",
    "for i in range(50):\n",
    "    y = model_trt(data)\n",
    "torch.cuda.current_stream().synchronize()\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.818953663363025\n"
     ]
    }
   ],
   "source": [
    "print(50.0 / (t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]\n",
    "\n",
    "def execute(img, src, t):\n",
    "    color = (0, 255, 0)\n",
    "    data = preprocess(img)\n",
    "    cmap, paf = model_trt(data)\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)#, cmap_threshold=0.15, link_threshold=0.15)\n",
    "    fps = 1.0 / (time.time() - t)\n",
    "    for i in range(counts[0]):\n",
    "        keypoints = get_keypoint(objects, i, peaks)\n",
    "        for j in range(len(keypoints)):\n",
    "            if keypoints[j][1]:\n",
    "                x = round(keypoints[j][2] * WIDTH * X_compress)\n",
    "                y = round(keypoints[j][1] * HEIGHT * Y_compress)\n",
    "                cv2.circle(src, (x, y), 3, color, 2)\n",
    "                cv2.putText(src , \"%d\" % int(keypoints[j][0]), (x + 5, y),  cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 1)\n",
    "                cv2.circle(src, (x, y), 3, color, 2)\n",
    "    print(\"FPS:%f \"%(fps))\n",
    "    #draw_objects(img, counts, objects, peaks)\n",
    "\n",
    "    cv2.putText(src , \"FPS: %f\" % (fps), (20, 20),  cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)\n",
    "    out_video.write(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "There's functionality to draw the keypoints on the video, but I'm ignoring it\n",
    "'''\n",
    "def execute_2(img, org, count, outpath):\n",
    "    start = time.time()\n",
    "    data = preprocess(img)\n",
    "    cmap, paf = model_trt(data)\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    end = time.time()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)#, cmap_threshold=0.15, link_threshold=0.15)\n",
    "    for i in range(counts[0]):\n",
    "        #print(\"Human index:%d \"%( i ))\n",
    "        kpoint = get_keypoint(objects, i, peaks)\n",
    "        ti = time.time()\n",
    "#         print(kpoint)\n",
    "        org = draw_keypoints(org, kpoint)\n",
    "    netfps = 1 / (end - start)  \n",
    "    draw = PIL.ImageDraw.Draw(org)\n",
    "    f = open(outpath, \"a\")\n",
    "    f.write(f\"{ti}\\t{kpoint}\\t{count}\\t{netfps}\\n\")\n",
    "#     f.close()\n",
    "#     draw.text((30, 30), \"NET FPS:%4.1f\"%netfps, font=fnt, fill=(0,255,0))    \n",
    "#     print(\"Human count:%d len:%d \"%(counts[0], len(counts)))\n",
    "#     print('===== Frame[%d] Net FPS :%f ====='%(count, netfps))\n",
    "    return org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pose(video_path, video_name):\n",
    "    cap = cv2.VideoCapture(video_path+video_name+\".mp4\")\n",
    "    # cap = cv2.VideoCapture(-1)\n",
    "    ret_val, img = cap.read()\n",
    "    H, W, __ = img.shape\n",
    "\n",
    "#     fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "    # dir, filename = os.path.split(args.video)\n",
    "    # name, ext = os.path.splitext(filename)\n",
    "    # out_video = cv2.VideoWriter('/home/spypiggy/src/test_images/result/%s_%s.mp4'%(args.model, name), fourcc, cap.get(cv2.CAP_PROP_FPS), (W, H))\n",
    "#     out_video = cv2.VideoWriter(\"../results/\"+video_name, fourcc, cap.get(cv2.CAP_PROP_FPS), (W, H))\n",
    "#     count = 0\n",
    "\n",
    "    X_compress = 640.0 / WIDTH * 1.0\n",
    "    Y_compress = 480.0 / HEIGHT * 1.0\n",
    "\n",
    "    if cap is None:\n",
    "        print(\"Camera Open Error\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    parse_objects = ParseObjects(topology)\n",
    "    draw_objects = DrawObjects(topology)\n",
    "\n",
    "    fontname = '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc'\n",
    "\n",
    "    # fnt = PIL.ImageFont.truetype(fontname, 45)\n",
    "\n",
    "    count = 1\n",
    "    outpath = \"../keypoints/\"+video_name+\".txt\"\n",
    "    while cap.isOpened():\n",
    "        ret_val, dst = cap.read()\n",
    "        if ret_val == False:\n",
    "            print(\"Frame Read End\")\n",
    "            break\n",
    "        img = cv2.resize(dst, dsize=(WIDTH, HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "        pilimg = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\n",
    "        pilimg = PIL.Image.fromarray(pilimg)\n",
    "        pilimg = execute_2(img, pilimg, count, outpath) \n",
    "        array = np.asarray(pilimg, dtype=\"uint8\")\n",
    "#         out_video.write(array) Put this back to write video\n",
    "        count += 1\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    out_video.release()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"../data/\"\n",
    "video_name = \"14_front_kw_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliplist = glob.glob(video_path + '*.mp4')\n",
    "cliplist = [x.replace(video_path, \"\").replace(\".mp4\", \"\") for x in cliplist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['40_front_kw_4',\n",
       " '18_front_kw_2',\n",
       " '18_front_kw_1',\n",
       " '20_side_kw_1',\n",
       " '40_front_kw_2',\n",
       " '18_front_kw_4',\n",
       " '20_front_kw_2',\n",
       " '14_front_kw_4',\n",
       " '14_front_kw_3',\n",
       " '20_side_kw_2',\n",
       " '20_front_kw_3',\n",
       " '18_front_kw_3',\n",
       " '20_front_kw_1',\n",
       " '40_front_kw_3',\n",
       " '40_front_kw_1',\n",
       " '20_side_kw_4',\n",
       " '20_side_kw_3',\n",
       " '14_front_kw_1',\n",
       " '14_front_kw_2']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cliplist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n",
      "Frame Read End\n"
     ]
    }
   ],
   "source": [
    "for clip in cliplist:\n",
    "    run_pose(video_path, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
